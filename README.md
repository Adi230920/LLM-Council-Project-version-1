<div align="center">

# BouleAI ‚Äî LLM Advisory Council

### *Four minds. Three stages. One verdict.*

[![Python](https://img.shields.io/badge/Python-3.12-3776AB?style=flat-square&logo=python&logoColor=white)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.111-009688?style=flat-square&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)
[![Render](https://img.shields.io/badge/Backend-Render%20Free-46E3B7?style=flat-square)](https://render.com/)
[![Vercel](https://img.shields.io/badge/Frontend-Vercel%20Free-000000?style=flat-square&logo=vercel&logoColor=white)](https://vercel.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=flat-square)](LICENSE)

</div>

---

## üìñ What is BouleAI?

Most AI chatbots give you **one opinion from one model**. BouleAI is different.

Inspired by ancient Athenian democratic councils (*Boule* ‚Äî Œ≤ŒøœÖŒªŒÆ), BouleAI routes every query through a **rigorous 3-stage deliberation pipeline** powered by multiple independent LLM models. Instead of trusting a single AI's answer, you get a synthesized consensus ‚Äî the product of independent reasoning, anonymous peer critique, and chairman synthesis.

---

## ‚òÅÔ∏è Deployment Architecture (100% Free)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         USER'S BROWSER                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ  Page Load           ‚îÇ  API Call (fetch)
                   ‚ñº                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   VERCEL FREE (CDN)      ‚îÇ   ‚îÇ   RENDER FREE (Web Service)          ‚îÇ
‚îÇ   frontend/index.html    ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ   FastAPI ‚Äî main.py                  ‚îÇ
‚îÇ   frontend/css/*.css     ‚îÇ   ‚îÇ   POST /api/v1/consult               ‚îÇ
‚îÇ   frontend/js/*.js       ‚îÇ   ‚îÇ   No timeout limit                   ‚îÇ
‚îÇ                          ‚îÇ   ‚îÇ   OpenRouter + Groq LLM calls        ‚îÇ
‚îÇ  (served globally from   ‚îÇ   ‚îÇ  (runs uvicorn, stays alive)         ‚îÇ
‚îÇ   Vercel Edge CDN)       ‚îÇ   ‚îÇ                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        FREE                              FREE
   Next-day deploy                  ~50s cold start on
   Instant CDN                      first req after idle
```

**Why this split?** Vercel Free has a 10-second function timeout ‚Äî far too short for the 3-stage LLM pipeline (30‚Äì90 s). Render Free has **no timeout** on web services. Vercel Free is ideal for the static frontend. Zero cost, no compromises.

---

## ‚ú® Features

- **3-Stage Deliberation Pipeline** ‚Äî Opinions ‚Üí Peer Review ‚Üí Chairman Synthesis
- **Multi-Provider Support** ‚Äî OpenRouter (free models) + Groq (fast inference)
- **Fully Async** ‚Äî Zero blocking calls; built on `aiohttp` + FastAPI ASGI
- **Graceful Degradation** ‚Äî Pipeline continues if individual models fail
- **Rate Limited** ‚Äî 5 req/min/IP on `/consult`, 20/min global
- **Zero Frontend Dependencies** ‚Äî Pure HTML5 / CSS3 / ES6 (no build step, no npm)
- **100% Free Hosting** ‚Äî Render Free backend + Vercel Free frontend

---

## üõ†Ô∏è Tech Stack

| Layer | Technology |
|---|---|
| **Backend** | FastAPI 0.111+ (Python 3.12, ASGI) |
| **LLM Providers** | OpenRouter (free-tier models) + Groq (ultra-fast) |
| **HTTP Client** | `aiohttp` (fully async, connection-pooled) |
| **Rate Limiting** | `slowapi` (in-memory, per-IP) |
| **Frontend** | Vanilla HTML5 + CSS3 + ES6 JS |
| **Backend Hosting** | [Render](https://render.com) Free Web Service |
| **Frontend Hosting** | [Vercel](https://vercel.com) Free Static CDN |

---

## üìÅ Project Structure

```
LLM-Council-Project-version-1/
‚îÇ
‚îú‚îÄ‚îÄ frontend/                 # ‚Üê Deployed to Vercel Free (static CDN)
‚îÇ   ‚îú‚îÄ‚îÄ index.html
‚îÇ   ‚îú‚îÄ‚îÄ css/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ variables.css
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ layout.css
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ components.css
‚îÇ   ‚îî‚îÄ‚îÄ js/
‚îÇ       ‚îú‚îÄ‚îÄ config.js         # ‚Üê SET YOUR RENDER URL HERE before Vercel deploy
‚îÇ       ‚îú‚îÄ‚îÄ api.js            # Reads config.js, calls Render backend
‚îÇ       ‚îú‚îÄ‚îÄ app.js
‚îÇ       ‚îú‚îÄ‚îÄ state.js
‚îÇ       ‚îî‚îÄ‚îÄ ui.js
‚îÇ
‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îî‚îÄ‚îÄ api.py                # POST /api/v1/consult, GET /api/v1/config
‚îÇ
‚îú‚îÄ‚îÄ services/                 # ‚Üê Deployed to Render Free (backend)
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator.py       # 3-stage pipeline coordinator
‚îÇ   ‚îú‚îÄ‚îÄ council_service.py
‚îÇ   ‚îú‚îÄ‚îÄ review_service.py
‚îÇ   ‚îú‚îÄ‚îÄ chairman_service.py
‚îÇ   ‚îú‚îÄ‚îÄ provider_manager.py   # Lazy-loaded client routing
‚îÇ   ‚îú‚îÄ‚îÄ openrouter_client.py
‚îÇ   ‚îî‚îÄ‚îÄ groq_client.py
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ schemas.py            # Pydantic models
‚îÇ
‚îú‚îÄ‚îÄ main.py                   # FastAPI app factory + CORS + middleware
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ Procfile                  # Render start command
‚îú‚îÄ‚îÄ runtime.txt               # python-3.12
‚îú‚îÄ‚îÄ vercel.json               # Vercel static frontend config
‚îú‚îÄ‚îÄ .env.example              # Env var template (safe to commit)
‚îî‚îÄ‚îÄ .gitignore
```

---

## ‚ö° The 3-Stage Pipeline

| Stage | Activity | Description |
|:---|:---|:---|
| **Stage 1** | **Opinions** | 4 LLM models respond independently in parallel |
| **Stage 2** | **Peer Review** | Council members anonymously score each other's reasoning |
| **Stage 3** | **Synthesis** | Chairman model reads all opinions + scores ‚Üí final verdict |

---

## üöÄ Local Development

### Prerequisites
- Python 3.12+
- Free API keys from [OpenRouter](https://openrouter.ai/keys) and [Groq](https://console.groq.com/keys)

```bash
# 1. Clone
git clone https://github.com/Adi230920/LLM-Council-Project-version-1.git
cd LLM-Council-Project-version-1

# 2. Virtual environment
python -m venv venv
venv\Scripts\activate       # Windows
# source venv/bin/activate  # macOS/Linux

# 3. Dependencies
pip install -r requirements.txt

# 4. Environment variables
copy .env.example .env      # Windows
# cp .env.example .env      # macOS/Linux
# ‚Üí Edit .env with your real API keys. Set ENVIRONMENT=development

# 5. Run
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

Open **http://localhost:8000** ‚Äî FastAPI serves both the backend API and the static frontend from one process.

> **Note:** `frontend/js/config.js` has `window.BOULE_BACKEND_URL = ""` by default, which makes `api.js` use relative paths (correct for local dev). Don't change this until you deploy to Render.

---

## üîê Environment Variables

Set these in **Render Dashboard ‚Üí Environment** (not Vercel ‚Äî Vercel only hosts static files):

| Variable | Required | Description |
|---|---|---|
| `OPENROUTER_API_KEY` | ‚úÖ | [openrouter.ai/keys](https://openrouter.ai/keys) ‚Äî free account |
| `GROQ_API_KEY` | ‚úÖ | [console.groq.com/keys](https://console.groq.com/keys) ‚Äî free account |
| `ENVIRONMENT` | ‚úÖ | Always `production` on Render |
| `ALLOWED_ORIGINS` | ‚úÖ | Your Vercel frontend URL, e.g. `https://bouleai.vercel.app` |

---

## ‚òÅÔ∏è Production Deployment (100% Free)

See [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) for the complete step-by-step walkthrough.

**Two-step summary:**
1. **Deploy backend ‚Üí Render Free** (no timeout, long-running FastAPI process)
2. **Deploy frontend ‚Üí Vercel Free** (instant CDN, global edge)

---

## üõ°Ô∏è Security

| Feature | Details |
|---|---|
| Rate Limiting | 5 req/min/IP on `/consult`, 20/min global |
| Security Headers | Nosniff, Frame-Options DENY, Referrer-Policy |
| CORS | Restricted to `ALLOWED_ORIGINS` env var |
| Prompt Cap | 800 chars max |
| Token Cap | 512 tokens max per model |

---

## üîß Troubleshooting

**`[Error: OPENROUTER_API_KEY is not configured]` in the response**
‚Üí API keys not set on Render. Go to Render Dashboard ‚Üí Your Service ‚Üí Environment ‚Üí add the keys.

**"Consult the Council" button does nothing / network error in browser console**
‚Üí `BOULE_BACKEND_URL` in `frontend/js/config.js` is empty or wrong. Update it with your Render URL and redeploy to Vercel.

**CORS error in browser console**
‚Üí `ALLOWED_ORIGINS` on Render does not match your Vercel URL. Update it in Render Environment and redeploy.

**Council takes 40‚Äì90 seconds to respond**
‚Üí Normal. The 3-stage pipeline calls 4+ LLM APIs. If the first request after a long idle takes longer, that's Render's free-tier cold start (~50 s). Subsequent requests are fast.

**`uvicorn: command not found` on Render build**
‚Üí Render must be running from the repo root. Check that `requirements.txt` is in the root directory.

---

## üó∫Ô∏è Future Improvements

- [ ] Streaming Stage 1 opinions to frontend as they arrive
- [ ] Persistent deliberation history (PostgreSQL)
- [ ] Model selection UI (let users pick council members)
- [ ] Export full trace as JSON / PDF
- [ ] User authentication

---

<div align="center">
Built with ‚ö° FastAPI ¬∑ üß† OpenRouter + Groq ¬∑ ‚òÅÔ∏è Render + Vercel (100% Free)
</div>
