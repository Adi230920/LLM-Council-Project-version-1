<div align="center">

# BouleAI ‚Äî LLM Advisory Council

### *Four minds. Three stages. One verdict.*

[![Python](https://img.shields.io/badge/Python-3.12-3776AB?style=flat-square&logo=python&logoColor=white)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.111-009688?style=flat-square&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)
[![OpenRouter](https://img.shields.io/badge/OpenRouter-Free%20Tier-FF6B35?style=flat-square)](https://openrouter.ai/)
[![Groq](https://img.shields.io/badge/Groq-Ultra--Fast%20LLM-F55036?style=flat-square)](https://groq.com/)
[![Vercel](https://img.shields.io/badge/Vercel-Ready-000000?style=flat-square&logo=vercel&logoColor=white)](https://vercel.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=flat-square)](LICENSE)

</div>

---

## üìñ What is BouleAI?

Most AI chatbots give you **one opinion from one model**. BouleAI is different.

Inspired by ancient Athenian democratic councils (*Boule* ‚Äî Œ≤ŒøœÖŒªŒÆ), BouleAI routes every query through a **rigorous 3-stage deliberation pipeline** powered by multiple independent LLM models. Instead of trusting a single AI's answer, you get a synthesized consensus ‚Äî the product of independent reasoning, anonymous peer critique, and a final chairman synthesis.

Think of it as a **peer-reviewed answer**, produced in real time.

---

## ‚ú® Features

- **3-Stage Deliberation Pipeline** ‚Äî Opinions ‚Üí Peer Review ‚Üí Chairman Synthesis
- **Multi-Provider Support** ‚Äî Routes requests to both OpenRouter and Groq simultaneously
- **Fully Async** ‚Äî Zero blocking calls; built on `aiohttp` and FastAPI's ASGI runtime
- **Graceful Degradation** ‚Äî If any model fails, the pipeline continues with the remaining results
- **Production-Hardened** ‚Äî Rate limiting (5 req/min/IP), CSP headers, anonymized peer reviews
- **Zero Frontend Dependencies** ‚Äî Pure HTML5 / CSS3 / ES6 (no build step, no npm)
- **Vercel-Ready** ‚Äî Serverless-compatible Python entrypoint with correct routing configuration

---

## üõ†Ô∏è Tech Stack

| Layer | Technology |
|---|---|
| **Backend** | FastAPI 0.111+ (Python 3.12, ASGI) |
| **LLM Providers** | OpenRouter (free-tier models) + Groq (ultra-fast inference) |
| **HTTP Client** | `aiohttp` (fully async, connection-pooled) |
| **Rate Limiting** | `slowapi` (in-memory, per-IP, no Redis required) |
| **Frontend** | Vanilla HTML5 + CSS3 + ES6 JavaScript |
| **Deployment** | Vercel (serverless Python runtime) |
| **Environment** | `python-dotenv` for local secrets management |

---

## üìÅ Project Structure

```
LLM-Council-Project-version-1/
‚îÇ
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îî‚îÄ‚îÄ index.py              # ‚Üê Vercel serverless entrypoint (re-exports FastAPI app)
‚îÇ
‚îú‚îÄ‚îÄ frontend/                 # ‚Üê Static files served directly by Vercel CDN
‚îÇ   ‚îú‚îÄ‚îÄ index.html
‚îÇ   ‚îú‚îÄ‚îÄ css/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ variables.css     # Design tokens (colors, spacing, typography)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ layout.css        # Page structure & responsive grid
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ components.css    # UI components (chat bubbles, cards, buttons)
‚îÇ   ‚îî‚îÄ‚îÄ js/
‚îÇ       ‚îú‚îÄ‚îÄ api.js            # Fetch calls to /api/v1/consult
‚îÇ       ‚îú‚îÄ‚îÄ app.js            # Application bootstrap & event wiring
‚îÇ       ‚îú‚îÄ‚îÄ state.js          # Lightweight state management
‚îÇ       ‚îî‚îÄ‚îÄ ui.js             # DOM rendering (chat messages, stage visualizer)
‚îÇ
‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îî‚îÄ‚îÄ api.py                # FastAPI Router ‚Äî POST /api/v1/consult, GET /api/v1/config
‚îÇ
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator.py       # 3-stage pipeline coordinator
‚îÇ   ‚îú‚îÄ‚îÄ council_service.py    # Stage 1: parallel opinion generation
‚îÇ   ‚îú‚îÄ‚îÄ review_service.py     # Stage 2: anonymous peer review
‚îÇ   ‚îú‚îÄ‚îÄ chairman_service.py   # Stage 3: synthesis & verdict
‚îÇ   ‚îú‚îÄ‚îÄ provider_manager.py   # Routes requests to OpenRouter or Groq
‚îÇ   ‚îú‚îÄ‚îÄ openrouter_client.py  # Async OpenRouter API client (retry + backoff)
‚îÇ   ‚îî‚îÄ‚îÄ groq_client.py        # Async Groq API client (retry + backoff)
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ schemas.py            # Pydantic request/response models
‚îÇ
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ (anonymization & security helpers)
‚îÇ
‚îú‚îÄ‚îÄ main.py                   # FastAPI app factory, middleware, static file mount
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ vercel.json               # Vercel build + routing configuration
‚îú‚îÄ‚îÄ runtime.txt               # Python version declaration (python-3.12)
‚îú‚îÄ‚îÄ Procfile                  # Render/Railway deployment command
‚îú‚îÄ‚îÄ .env.example              # Environment variable template (safe to commit)
‚îî‚îÄ‚îÄ .gitignore                # Excludes .env, __pycache__, venv, etc.
```

---

## ‚ö° The 3-Stage Pipeline

```
User Query
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STAGE 1 ‚Äî Independent Opinions                             ‚îÇ
‚îÇ  4 LLM models (OpenRouter free-tier) respond in parallel   ‚îÇ
‚îÇ  Each model sees only the original prompt, not each other  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STAGE 2 ‚Äî Anonymous Peer Review                            ‚îÇ
‚îÇ  Council members cross-review each other's responses       ‚îÇ
‚îÇ  Responses are anonymized (Response #1, #2...) to reduce   ‚îÇ
‚îÇ  model-identity bias in scoring                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STAGE 3 ‚Äî Chairman Synthesis                               ‚îÇ
‚îÇ  A designated "Chairman" model reads all opinions +        ‚îÇ
‚îÇ  reviews, resolves contradictions, and synthesizes a       ‚îÇ
‚îÇ  final consensus verdict                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
Structured JSON Response (DeliberationTrace)
```

---

## üöÄ Installation & Local Development

### Prerequisites

- Python 3.12+
- API keys for [OpenRouter](https://openrouter.ai/keys) (free) and optionally [Groq](https://console.groq.com/keys) (free)

### Step 1 ‚Äî Clone the Repository

```bash
git clone https://github.com/Adi230920/LLM-Council-Project-version-1.git
cd LLM-Council-Project-version-1
```

### Step 2 ‚Äî Create a Virtual Environment

```bash
# Create
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Activate (macOS / Linux)
source venv/bin/activate
```

### Step 3 ‚Äî Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 4 ‚Äî Configure Environment Variables

```bash
# Copy the example file
cp .env.example .env   # macOS/Linux
copy .env.example .env  # Windows
```

Then open `.env` and fill in your real API keys:

```toml
OPENROUTER_API_KEY=sk-or-v1-your-real-key-here
GROQ_API_KEY=gsk_your-real-key-here
ENVIRONMENT=development
ALLOWED_ORIGINS=http://localhost:8000
```

### Step 5 ‚Äî Start the Server

```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

Open **[http://localhost:8000](http://localhost:8000)** in your browser to consult the council.

> **Tip:** With `ENVIRONMENT=development`, the FastAPI docs are available at [http://localhost:8000/docs](http://localhost:8000/docs).

---

## üîê Environment Variables

| Variable | Required | Description |
|---|---|---|
| `OPENROUTER_API_KEY` | ‚úÖ Yes | OpenRouter API key ‚Äî get one free at [openrouter.ai/keys](https://openrouter.ai/keys) |
| `GROQ_API_KEY` | ‚úÖ Yes | Groq API key ‚Äî get one free at [console.groq.com/keys](https://console.groq.com/keys) |
| `ENVIRONMENT` | ‚úÖ Yes | `development` (enables `/docs`) or `production` (disables docs) |
| `ALLOWED_ORIGINS` | ‚úÖ Yes | Comma-separated list of allowed CORS origins (e.g. `https://myapp.vercel.app`) |

---

## üèóÔ∏è Build Instructions

**This project has no build step.** The frontend is pure HTML/CSS/JS served directly as static files.

```bash
# Verify all dependencies install cleanly
pip install -r requirements.txt

# Verify the application imports correctly
python -c "from main import app; print('‚úÖ App import OK')"

# Run in production mode (single worker for serverless)
uvicorn main:app --host 0.0.0.0 --port 8000 --workers 1
```

---

## ‚òÅÔ∏è Production Deployment

### Deploying to Vercel (Full-Stack Serverless)

See [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) for the complete step-by-step walkthrough.

**Quick summary:**

1. Push this repo to GitHub
2. Go to [vercel.com/new](https://vercel.com/new) and import the repo
3. Add your 4 environment variables in Vercel Dashboard ‚Üí Settings ‚Üí Environment Variables
4. Click **Deploy**

The `vercel.json` file in this repo handles all build + routing configuration automatically.

> ‚ö†Ô∏è **Timeout Note:** The 3-stage LLM pipeline can take 30‚Äì90 seconds. Vercel's `maxDuration: 60` setting (used in this project's `vercel.json`) requires the **Pro plan**. On the free Hobby plan, requests will time out after 10 seconds. Consider using [Render](https://render.com/) for the backend if you're on a free tier.

### Deploying to Render (Long-Running Process)

Render supports the `Procfile` already included in this repo:

```
web: uvicorn main:app --host 0.0.0.0 --port $PORT --workers 1
```

1. Connect the repo in your Render dashboard
2. Set the environment variables
3. Deploy

---

## üõ°Ô∏è Security & Rate Limiting

| Feature | Implementation |
|---|---|
| **Rate Limiting** | 5 requests/min per IP on `/api/v1/consult` via `slowapi` |
| **Global Limit** | 20 requests/min per IP across all routes |
| **Security Headers** | X-Content-Type-Options, X-Frame-Options DENY, CSP |
| **CORS** | Restricted to origins listed in `ALLOWED_ORIGINS` |
| **Prompt Cap** | 800 characters max per request (prevents abuse) |
| **Token Cap** | 512 tokens max per model response |

---

## üîß Troubleshooting

### `EnvironmentError: OPENROUTER_API_KEY is not set`
Ensure your `.env` file exists in the project root (same directory as `main.py`) and is fully populated. Run `cat .env` to confirm. On Vercel, check your Environment Variables in the dashboard.

### `502 Bad Gateway` on the `/api/v1/consult` endpoint
The council deliberation timed out. This usually means either (a) the free-tier LLM models are under heavy load, or (b) you're on Vercel Hobby plan with a 10s timeout. Retry after a minute, or upgrade to Vercel Pro.

### The `/docs` page returns 404
`/docs` is only enabled when `ENVIRONMENT=development`. Set this in your `.env` for local development.

### `Cannot connect to localhost:8000` after starting the server
Ensure the virtual environment is activated (`venv\Scripts\activate` on Windows) and that you've installed requirements (`pip install -r requirements.txt`).

---

## üó∫Ô∏è Future Improvements

- [ ] **Streaming responses** ‚Äî stream Stage 1 opinions to the frontend as they arrive instead of waiting for all 4
- [ ] **Persistent history** ‚Äî save deliberation traces to a database (PostgreSQL via SQLAlchemy)
- [ ] **Model selection UI** ‚Äî let users pick which council models to use
- [ ] **Custom Chairman** ‚Äî allow users to designate a more powerful model (e.g. GPT-4o) as Chairman
- [ ] **Export trace** ‚Äî download the full deliberation trace as a PDF or JSON file
- [ ] **Authentication** ‚Äî user accounts to track and revisit past councils
- [ ] **Redis rate limiting** ‚Äî replace in-memory rate limiter for multi-instance deployments

---

## üìÑ License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.

---

<div align="center">
Built with ‚ö° FastAPI, üß† OpenRouter + Groq, and a commitment to epistemic rigor.
</div>
